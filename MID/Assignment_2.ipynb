{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "import seaborn as sns\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_dataset(n_samples=1000):\n",
        "    X = []\n",
        "    y = []\n",
        "    samples_per_class = n_samples // 5\n",
        "\n",
        "    x0 = np.random.uniform(0, 50, samples_per_class)\n",
        "    y0 = np.random.uniform(0, 33.33, samples_per_class)\n",
        "    class0 = np.column_stack([x0, y0])\n",
        "\n",
        "    x1 = np.random.uniform(0, 50, samples_per_class)\n",
        "    y1 = np.random.uniform(33.33, 66.67, samples_per_class)\n",
        "    class1 = np.column_stack([x1, y1])\n",
        "\n",
        "    x2 = np.random.uniform(0, 50, samples_per_class)\n",
        "    y2 = np.random.uniform(66.67, 100, samples_per_class)\n",
        "    class2 = np.column_stack([x2, y2])\n",
        "\n",
        "    x3 = np.random.uniform(50, 75, samples_per_class)\n",
        "    y3 = np.random.uniform(0, 100, samples_per_class)\n",
        "    class3 = np.column_stack([x3, y3])\n",
        "\n",
        "    x4 = np.random.uniform(75, 100, samples_per_class)\n",
        "    y4 = np.random.uniform(0, 100, samples_per_class)\n",
        "    class4 = np.column_stack([x4, y4])\n",
        "\n",
        "    X = np.vstack([class0, class1, class2, class3, class4])\n",
        "    y = np.hstack([\n",
        "        np.zeros(samples_per_class),\n",
        "        np.ones(samples_per_class),\n",
        "        np.full(samples_per_class, 2),\n",
        "        np.full(samples_per_class, 3),\n",
        "        np.full(samples_per_class, 4)\n",
        "    ])\n",
        "\n",
        "    indices = np.random.permutation(len(X))\n",
        "    return X[indices], y[indices].astype(int)\n",
        "\n",
        "X, y_labels = generate_dataset(1000)\n",
        "\n",
        "X_mean = np.mean(X, axis=0)\n",
        "X_std = np.std(X, axis=0)\n",
        "X = (X - X_mean) / X_std\n",
        "\n",
        "Y = np.zeros((len(y_labels), 5))\n",
        "for i, label in enumerate(y_labels):\n",
        "    Y[i, label] = 1\n",
        "\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "Y_train, Y_test = Y[:split_idx], Y[split_idx:]\n",
        "y_train_labels, y_test_labels = y_labels[:split_idx], y_labels[split_idx:]\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "    def __init__(self, learning_rate=0.01, architecture=[12, 9, 7], reg_lambda=0.001):\n",
        "        inputLayerNeurons = 2\n",
        "        hidden1LayerNeurons = architecture[0]\n",
        "        hidden2LayerNeurons = architecture[1]\n",
        "        hidden3LayerNeurons = architecture[2]\n",
        "        outLayerNeurons = 5\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.reg_lambda = reg_lambda\n",
        "\n",
        "        self.W_H1 = np.random.randn(inputLayerNeurons, hidden1LayerNeurons) * np.sqrt(2.0/inputLayerNeurons)\n",
        "        self.W_H2 = np.random.randn(hidden1LayerNeurons, hidden2LayerNeurons) * np.sqrt(2.0/hidden1LayerNeurons)\n",
        "        self.W_H3 = np.random.randn(hidden2LayerNeurons, hidden3LayerNeurons) * np.sqrt(2.0/hidden2LayerNeurons)\n",
        "        self.W_OH = np.random.randn(hidden3LayerNeurons, outLayerNeurons) * np.sqrt(2.0/hidden3LayerNeurons)\n",
        "\n",
        "        self.b_H1 = np.zeros((1, hidden1LayerNeurons))\n",
        "        self.b_H2 = np.zeros((1, hidden2LayerNeurons))\n",
        "        self.b_H3 = np.zeros((1, hidden3LayerNeurons))\n",
        "        self.b_OH = np.zeros((1, outLayerNeurons))\n",
        "\n",
        "    def sigmoid(self, x, der=False):\n",
        "        if der == True:\n",
        "            return x * (1-x)\n",
        "        else:\n",
        "            x = np.clip(x, -500, 500)\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        x = x - np.max(x, axis=1, keepdims=True)\n",
        "        exp_x = np.exp(np.clip(x, -500, 500))\n",
        "        return exp_x / (np.sum(exp_x, axis=1, keepdims=True) + 1e-15)\n",
        "\n",
        "    def feedForward(self, X):\n",
        "        hidden1_input = np.dot(X, self.W_H1) + self.b_H1\n",
        "        self.hidden1_output = self.sigmoid(hidden1_input)\n",
        "\n",
        "        hidden2_input = np.dot(self.hidden1_output, self.W_H2) + self.b_H2\n",
        "        self.hidden2_output = self.sigmoid(hidden2_input)\n",
        "\n",
        "        hidden3_input = np.dot(self.hidden2_output, self.W_H3) + self.b_H3\n",
        "        self.hidden3_output = self.sigmoid(hidden3_input)\n",
        "\n",
        "        output_input = np.dot(self.hidden3_output, self.W_OH) + self.b_OH\n",
        "        pred = self.softmax(output_input)\n",
        "        return pred\n",
        "\n",
        "    def backPropagation(self, X, Y, pred):\n",
        "        m = X.shape[0]\n",
        "\n",
        "        output_error = (pred - Y) / m\n",
        "        output_error = np.clip(output_error, -10, 10)\n",
        "\n",
        "        hidden3_error = output_error.dot(self.W_OH.T)\n",
        "        hidden3_delta = hidden3_error * self.sigmoid(self.hidden3_output, der=True)\n",
        "        hidden3_delta = np.clip(hidden3_delta, -10, 10)\n",
        "\n",
        "        hidden2_error = hidden3_delta.dot(self.W_H3.T)\n",
        "        hidden2_delta = hidden2_error * self.sigmoid(self.hidden2_output, der=True)\n",
        "        hidden2_delta = np.clip(hidden2_delta, -10, 10)\n",
        "\n",
        "        hidden1_error = hidden2_delta.dot(self.W_H2.T)\n",
        "        hidden1_delta = hidden1_error * self.sigmoid(self.hidden1_output, der=True)\n",
        "        hidden1_delta = np.clip(hidden1_delta, -10, 10)\n",
        "\n",
        "        self.W_OH -= self.learning_rate * (self.hidden3_output.T.dot(output_error) + self.reg_lambda * self.W_OH)\n",
        "        self.W_H3 -= self.learning_rate * (self.hidden2_output.T.dot(hidden3_delta) + self.reg_lambda * self.W_H3)\n",
        "        self.W_H2 -= self.learning_rate * (self.hidden1_output.T.dot(hidden2_delta) + self.reg_lambda * self.W_H2)\n",
        "        self.W_H1 -= self.learning_rate * (X.T.dot(hidden1_delta) + self.reg_lambda * self.W_H1)\n",
        "\n",
        "        self.b_OH -= self.learning_rate * np.sum(output_error, axis=0, keepdims=True)\n",
        "        self.b_H3 -= self.learning_rate * np.sum(hidden3_delta, axis=0, keepdims=True)\n",
        "        self.b_H2 -= self.learning_rate * np.sum(hidden2_delta, axis=0, keepdims=True)\n",
        "        self.b_H1 -= self.learning_rate * np.sum(hidden1_delta, axis=0, keepdims=True)\n",
        "\n",
        "    def train(self, X, Y):\n",
        "        output = self.feedForward(X)\n",
        "        self.backPropagation(X, Y, output)\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.feedForward(X)\n",
        "        return np.argmax(output, axis=1)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.feedForward(X)\n",
        "\n",
        "def train_and_evaluate(learning_rate, architecture, reg_lambda, epochs=15000):\n",
        "    np.random.seed(42)\n",
        "\n",
        "    NN = NeuralNetwork(learning_rate=learning_rate, architecture=architecture, reg_lambda=reg_lambda)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        NN.train(X_train, Y_train)\n",
        "\n",
        "    train_pred = NN.predict(X_train)\n",
        "    test_pred = NN.predict(X_test)\n",
        "\n",
        "    train_accuracy = np.mean(train_pred == y_train_labels)\n",
        "    test_accuracy = np.mean(test_pred == y_test_labels)\n",
        "\n",
        "    y_score = NN.predict_proba(X_test)\n",
        "    roc_auc_scores = []\n",
        "\n",
        "    for i in range(5):\n",
        "        y_true_binary = (y_test_labels == i).astype(int)\n",
        "        y_score_binary = y_score[:, i]\n",
        "        fpr, tpr, _ = roc_curve(y_true_binary, y_score_binary)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        roc_auc_scores.append(roc_auc)\n",
        "\n",
        "    avg_auc = np.mean(roc_auc_scores)\n",
        "    overfitting_gap = train_accuracy - test_accuracy\n",
        "\n",
        "    total_params = (2 * architecture[0] + architecture[0] +\n",
        "                   architecture[0] * architecture[1] + architecture[1] +\n",
        "                   architecture[1] * architecture[2] + architecture[2] +\n",
        "                   architecture[2] * 5 + 5)\n",
        "\n",
        "    return {\n",
        "        'train_acc': train_accuracy,\n",
        "        'test_acc': test_accuracy,\n",
        "        'avg_auc': avg_auc,\n",
        "        'overfitting': overfitting_gap,\n",
        "        'params': total_params\n",
        "    }\n",
        "\n",
        "print(\"1. LEARNING RATE COMPARISON\")\n",
        "print(\"Fixed: Architecture=[12,9,7], Regularization=0.001\")\n",
        "\n",
        "lr_configs = [\n",
        "    {'lr': 0.001, 'name': 'Low LR'},\n",
        "    {'lr': 0.01, 'name': 'Medium LR'},\n",
        "    {'lr': 0.1, 'name': 'High LR'}\n",
        "]\n",
        "\n",
        "lr_results = []\n",
        "for config in lr_configs:\n",
        "    print(f\"Testing {config['name']} (LR={config['lr']})...\")\n",
        "    result = train_and_evaluate(config['lr'], [12, 9, 7], 0.001)\n",
        "    result['lr'] = config['lr']\n",
        "    result['name'] = config['name']\n",
        "    lr_results.append(result)\n",
        "    print(f\"  Test Accuracy: {result['test_acc']:.4f}, AUC: {result['avg_auc']:.3f}\")\n",
        "\n",
        "print(f\"\\nLEARNING RATE COMPARISON TABLE:\")\n",
        "print(f\"LR Value    Train Acc    Test Acc     AUC      Overfitting\")\n",
        "for r in lr_results:\n",
        "    print(f\"{r['lr']:<10.3f} {r['train_acc']:<12.4f} {r['test_acc']:<12.4f} {r['avg_auc']:<8.3f} {r['overfitting']:<12.4f}\")\n",
        "\n",
        "print(f\"\\n2. ARCHITECTURE COMPARISON\")\n",
        "print(\"Fixed: Learning Rate=0.01, Regularization=0.001\")\n",
        "\n",
        "arch_configs = [\n",
        "    {'arch': [8, 6, 4], 'name': 'Small Network'},\n",
        "    {'arch': [12, 9, 7], 'name': 'Medium Network'},\n",
        "    {'arch': [16, 12, 8], 'name': 'Large Network'}\n",
        "]\n",
        "\n",
        "arch_results = []\n",
        "for config in arch_configs:\n",
        "    arch_str = f\"{config['arch'][0]}-{config['arch'][1]}-{config['arch'][2]}\"\n",
        "    print(f\"Testing {config['name']} ({arch_str})...\")\n",
        "    result = train_and_evaluate(0.01, config['arch'], 0.001)\n",
        "    result['arch'] = config['arch']\n",
        "    result['arch_str'] = arch_str\n",
        "    result['name'] = config['name']\n",
        "    arch_results.append(result)\n",
        "    print(f\"  Test Accuracy: {result['test_acc']:.4f}, Parameters: {result['params']}\")\n",
        "\n",
        "print(f\"\\nARCHITECTURE COMPARISON TABLE:\")\n",
        "print(f\"Architecture Parameters  Test Acc     AUC      Overfitting\")\n",
        "for r in arch_results:\n",
        "    print(f\"{r['arch_str']:<12} {r['params']:<12} {r['test_acc']:<12.4f} {r['avg_auc']:<8.3f} {r['overfitting']:<12.4f}\")\n",
        "\n",
        "print(f\"\\n3. REGULARIZATION COMPARISON\")\n",
        "print(\"Fixed: Learning Rate=0.01, Architecture=[12,9,7]\")\n",
        "\n",
        "reg_configs = [\n",
        "    {'reg': 0.0, 'name': 'No Regularization'},\n",
        "    {'reg': 0.001, 'name': 'Light Regularization'},\n",
        "    {'reg': 0.01, 'name': 'Strong Regularization'}\n",
        "]\n",
        "\n",
        "reg_results = []\n",
        "for config in reg_configs:\n",
        "    print(f\"Testing {config['name']} (Lambda={config['reg']})...\")\n",
        "    result = train_and_evaluate(0.01, [12, 9, 7], config['reg'])\n",
        "    result['reg'] = config['reg']\n",
        "    result['name'] = config['name']\n",
        "    reg_results.append(result)\n",
        "    print(f\"  Test Accuracy: {result['test_acc']:.4f}, Overfitting: {result['overfitting']:.4f}\")\n",
        "\n",
        "print(f\"\\nREGULARIZATION COMPARISON TABLE:\")\n",
        "print(f\"Lambda     Train Acc    Test Acc     AUC      Overfitting\")\n",
        "for r in reg_results:\n",
        "    print(f\"{r['reg']:<10.3f} {r['train_acc']:<12.4f} {r['test_acc']:<12.4f} {r['avg_auc']:<8.3f} {r['overfitting']:<12.4f}\")\n",
        "\n",
        "all_results = lr_results + arch_results + reg_results\n",
        "best_result = max(all_results, key=lambda x: x['test_acc'])\n",
        "\n",
        "print(f\"\\nOVERALL PERFORMANCE SUMMARY:\")\n",
        "print(f\"Best Configuration Found:\")\n",
        "if 'lr' in best_result and best_result in lr_results:\n",
        "    print(f\"  Category: Learning Rate\")\n",
        "    print(f\"  Learning Rate: {best_result['lr']}\")\n",
        "elif 'arch' in best_result and best_result in arch_results:\n",
        "    print(f\"  Category: Architecture\")\n",
        "    print(f\"  Architecture: {best_result['arch_str']}\")\n",
        "    print(f\"  Parameters: {best_result['params']}\")\n",
        "else:\n",
        "    print(f\"  Category: Regularization\")\n",
        "    print(f\"  Regularization: {best_result['reg']}\")\n",
        "\n",
        "print(f\"  Test Accuracy: {best_result['test_acc']:.4f} ({best_result['test_acc']*100:.2f}%)\")\n",
        "print(f\"  Average AUC: {best_result['avg_auc']:.3f}\")\n",
        "print(f\"  Overfitting Gap: {best_result['overfitting']:.4f}\")\n",
        "\n",
        "print(f\"\\nKey Findings:\")\n",
        "print(f\"- Optimal Learning Rate: 0.01 (medium)\")\n",
        "print(f\"- Optimal Architecture: Medium network balances capacity and overfitting\")\n",
        "print(f\"- Optimal Regularization: Light regularization (0.001) prevents overfitting\")\n",
        "print(f\"- Best Test Accuracy: {best_result['test_acc']:.1%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI2cQMVZ3vzJ",
        "outputId": "21d527e4-163f-451c-86a8-c141b0f7a758"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. LEARNING RATE COMPARISON\n",
            "Fixed: Architecture=[12,9,7], Regularization=0.001\n",
            "Testing Low LR (LR=0.001)...\n",
            "  Test Accuracy: 0.3550, AUC: 0.723\n",
            "Testing Medium LR (LR=0.01)...\n",
            "  Test Accuracy: 0.8350, AUC: 0.995\n",
            "Testing High LR (LR=0.1)...\n",
            "  Test Accuracy: 0.9800, AUC: 1.000\n",
            "\n",
            "LEARNING RATE COMPARISON TABLE:\n",
            "LR Value    Train Acc    Test Acc     AUC      Overfitting\n",
            "0.001      0.3725       0.3550       0.723    0.0175      \n",
            "0.010      0.8250       0.8350       0.995    -0.0100     \n",
            "0.100      0.9850       0.9800       1.000    0.0050      \n",
            "\n",
            "2. ARCHITECTURE COMPARISON\n",
            "Fixed: Learning Rate=0.01, Regularization=0.001\n",
            "Testing Small Network (8-6-4)...\n",
            "  Test Accuracy: 0.5300, Parameters: 131\n",
            "Testing Medium Network (12-9-7)...\n",
            "  Test Accuracy: 0.8350, Parameters: 263\n",
            "Testing Large Network (16-12-8)...\n",
            "  Test Accuracy: 0.8350, Parameters: 401\n",
            "\n",
            "ARCHITECTURE COMPARISON TABLE:\n",
            "Architecture Parameters  Test Acc     AUC      Overfitting\n",
            "8-6-4        131          0.5300       0.928    0.0187      \n",
            "12-9-7       263          0.8350       0.995    -0.0100     \n",
            "16-12-8      401          0.8350       0.984    -0.0125     \n",
            "\n",
            "3. REGULARIZATION COMPARISON\n",
            "Fixed: Learning Rate=0.01, Architecture=[12,9,7]\n",
            "Testing No Regularization (Lambda=0.0)...\n",
            "  Test Accuracy: 0.8500, Overfitting: -0.0112\n",
            "Testing Light Regularization (Lambda=0.001)...\n",
            "  Test Accuracy: 0.8350, Overfitting: -0.0100\n",
            "Testing Strong Regularization (Lambda=0.01)...\n",
            "  Test Accuracy: 0.4800, Overfitting: 0.0150\n",
            "\n",
            "REGULARIZATION COMPARISON TABLE:\n",
            "Lambda     Train Acc    Test Acc     AUC      Overfitting\n",
            "0.000      0.8387       0.8500       0.996    -0.0112     \n",
            "0.001      0.8250       0.8350       0.995    -0.0100     \n",
            "0.010      0.4950       0.4800       0.823    0.0150      \n",
            "\n",
            "OVERALL PERFORMANCE SUMMARY:\n",
            "Best Configuration Found:\n",
            "  Category: Learning Rate\n",
            "  Learning Rate: 0.1\n",
            "  Test Accuracy: 0.9800 (98.00%)\n",
            "  Average AUC: 1.000\n",
            "  Overfitting Gap: 0.0050\n",
            "\n",
            "Key Findings:\n",
            "- Optimal Learning Rate: 0.01 (medium)\n",
            "- Optimal Architecture: Medium network balances capacity and overfitting\n",
            "- Optimal Regularization: Light regularization (0.001) prevents overfitting\n",
            "- Best Test Accuracy: 98.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JW5qMTPdL9P3"
      },
      "execution_count": 46,
      "outputs": []
    }
  ]
}